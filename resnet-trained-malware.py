#!/usr/bin/env python3
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from tqdm import tqdm
from sklearn.metrics import f1_score, confusion_matrix
from PIL import Image
import numpy as np
import random

##############################
# Constants & Parameters
##############################
IMAGE_SIZE = 256         # Images are 256x256
BATCH_SIZE = 32
NUM_EPOCHS = 50
LEARNING_RATE = 0.001
PATIENCE = 10            # Early stopping patience
RANDOM_SEED = 42         # For reproducibility

# Paths to your pre-split datasets
train_malware_dir = "/Users/marq/Projects/binSleuth-CNN/splittedDataset/train/malware"
train_benign_dir = "/Users/marq/Projects/binSleuth-CNN/splittedDataset/train/benign"
val_malware_dir = "/Users/marq/Projects/binSleuth-CNN/splittedDataset/val/malware"
val_benign_dir = "/Users/marq/Projects/binSleuth-CNN/splittedDataset/val/benign"
test_malware_dir = "/Users/marq/Projects/binSleuth-CNN/splittedDataset/test/malware"  # Fixed path
test_benign_dir = "/Users/marq/Projects/binSleuth-CNN/splittedDataset/test/benign"

# Set random seeds for reproducibility
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(RANDOM_SEED)

##############################
# Device Helper Function
##############################
def get_device():
    if torch.cuda.is_available():
        return torch.device("cuda")
    elif torch.backends.mps.is_available():
        return torch.device("mps")
    else:
        return torch.device("cpu")

##############################
# Custom Dataset Class
##############################
class MalwareDataset(Dataset):
    def __init__(self, malware_dir, benign_dir, transform=None):
        self.malware_images = [
            os.path.join(malware_dir, img) 
            for img in os.listdir(malware_dir) 
            if img.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))
        ]
        self.benign_images = [
            os.path.join(benign_dir, img) 
            for img in os.listdir(benign_dir) 
            if img.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))
        ]
        self.image_paths = self.malware_images + self.benign_images
        self.labels = [1] * len(self.malware_images) + [0] * len(self.benign_images)
        self.transform = transform
        
    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        label = self.labels[idx]
        
        try:
            image = Image.open(img_path).convert("L")  # Convert to grayscale
            if self.transform:
                image = self.transform(image)
            return image, label
        except Exception as e:
            print(f"Error loading image {img_path}: {e}")
            # Return a placeholder image in case of error
            placeholder = torch.zeros((1, IMAGE_SIZE, IMAGE_SIZE))
            return placeholder, label

##############################
# Model Definition
##############################
class GrayscaleResNetModel(nn.Module):
    """
    ResNet-18 modified to accept 1-channel (grayscale) input and perform binary classification.
    """
    def __init__(self, num_classes=2, pretrained=True):
        super(GrayscaleResNetModel, self).__init__()
        self.model = models.resnet18(pretrained=pretrained)
        # Change first conv layer to accept 1 channel
        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
        num_features = self.model.fc.in_features
        self.model.fc = nn.Linear(num_features, num_classes)
        
    def forward(self, x):
        return self.model(x)

##############################
# Training Routine
##############################
def main():
    device = get_device()
    print(f"Using device: {device}")

    # Define transforms
    train_transforms = transforms.Compose([
        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    
    val_transforms = transforms.Compose([
        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])

    # Create datasets
    train_dataset = MalwareDataset(train_malware_dir, train_benign_dir, transform=train_transforms)
    val_dataset = MalwareDataset(val_malware_dir, val_benign_dir, transform=val_transforms)
    test_dataset = MalwareDataset(test_malware_dir, test_benign_dir, transform=val_transforms)
    
    # Print dataset statistics
    train_malware = len(train_dataset.malware_images)
    train_benign = len(train_dataset.benign_images)
    val_malware = len(val_dataset.malware_images)
    val_benign = len(val_dataset.benign_images)
    test_malware = len(test_dataset.malware_images)
    test_benign = len(test_dataset.benign_images)
    
    print(f"Training set: {len(train_dataset)} total - {train_malware} malware, {train_benign} benign")
    print(f"Validation set: {len(val_dataset)} total - {val_malware} malware, {val_benign} benign")
    print(f"Test set: {len(test_dataset)} total - {test_malware} malware, {test_benign} benign")
    
    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)
    
    # Handle class imbalance with weighted loss if needed
    if train_malware > 2 * train_benign or train_benign > 2 * train_malware:
        print("Applying class weights to handle imbalance")
        class_weights = torch.FloatTensor([
            len(train_dataset) / (2 * (len(train_dataset) - train_malware)),  # weight for benign
            len(train_dataset) / (2 * train_malware)  # weight for malware
        ]).to(device)
        criterion = nn.CrossEntropyLoss(weight=class_weights)
    else:
        criterion = nn.CrossEntropyLoss()

    # Initialize model, optimizer, scheduler
    model = GrayscaleResNetModel(num_classes=2, pretrained=True).to(device)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', patience=3, factor=0.1, verbose=True
    )

    best_val_loss = float('inf')
    patience_counter = 0

    for epoch in range(NUM_EPOCHS):
        # Training Phase
        model.train()
        running_loss = 0.0
        train_total = 0
        train_correct = 0
        train_preds = []
        train_labels_list = []

        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Train]")
        for inputs, labels in pbar:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            _, preds = torch.max(outputs, 1)
            train_total += labels.size(0)
            train_correct += (preds == labels).sum().item()
            train_preds.extend(preds.cpu().numpy())
            train_labels_list.extend(labels.cpu().numpy())
            pbar.set_postfix({"loss": f"{loss.item():.4f}", "acc": f"{train_correct/train_total:.4f}"})
        
        train_loss = running_loss / train_total
        train_acc = train_correct / train_total
        train_f1 = f1_score(train_labels_list, train_preds, average='binary', zero_division=1)

        # Validation Phase
        model.eval()
        val_running_loss = 0.0
        val_total = 0
        val_correct = 0
        all_preds = []
        all_labels = []
        
        pbar_val = tqdm(val_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Val]")
        with torch.no_grad():
            for inputs, labels in pbar_val:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_running_loss += loss.item() * inputs.size(0)
                _, preds = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (preds == labels).sum().item()
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                pbar_val.set_postfix({"loss": f"{loss.item():.4f}", "acc": f"{val_correct/val_total:.4f}"})
        
        val_loss = val_running_loss / val_total
        val_acc = val_correct / val_total
        val_f1 = f1_score(all_labels, all_preds, average='binary', zero_division=1)

        scheduler.step(val_loss)

        print(f"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f} | " +
              f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}")

        # Print confusion matrix
        cm = confusion_matrix(all_labels, all_preds)
        print(f"Validation Confusion Matrix:\n{cm}")

        # Check if model improved
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'val_acc': val_acc,
                'val_f1': val_f1,
            }, "best_model.pth")
            print(f"Model improved. Saved model with Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}")
        else:
            patience_counter += 1
            if patience_counter >= PATIENCE:
                print(f"Early stopping triggered at epoch {epoch+1}")
                break

    # Evaluate on test set using the best model
    print("\n--- Final Evaluation on Test Set ---")
    # Load the best model
    checkpoint = torch.load("best_model.pth")
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    test_total = 0
    test_correct = 0
    test_preds = []
    test_labels_list = []
    
    with torch.no_grad():
        for inputs, labels in tqdm(test_loader, desc="Testing"):
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            test_total += labels.size(0)
            test_correct += (preds == labels).sum().item()
            test_preds.extend(preds.cpu().numpy())
            test_labels_list.extend(labels.cpu().numpy())
    
    test_acc = test_correct / test_total
    test_f1 = f1_score(test_labels_list, test_preds, average='binary', zero_division=1)
    
    print(f"Test Accuracy: {test_acc:.4f}")
    print(f"Test F1 Score: {test_f1:.4f}")
    
    # Print confusion matrix for test set
    cm = confusion_matrix(test_labels_list, test_preds)
    print(f"Test Confusion Matrix:\n{cm}")
    
    # Save final stats
    torch.save({
        'model_state_dict': model.state_dict(),
        'test_acc': test_acc,
        'test_f1': test_f1,
        'confusion_matrix': cm.tolist(),
    }, "final_model.pth")
    
    print("Training and evaluation complete. Models saved.")

if __name__ == "__main__":
    main()